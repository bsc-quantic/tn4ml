{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection MNIST - MPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": {
     "title": "Anomaly detection with MPS"
    }
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import quimb.tensor as qtn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from jax.nn.initializers import *\n",
    "from flax.training.early_stopping import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tn4ml.initializers import *\n",
    "from tn4ml.models.mps import *\n",
    "from tn4ml.models.model import *\n",
    "from tn4ml.embeddings import *\n",
    "from tn4ml.loss import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = mnist.load_data()\n",
    "data = {\"X\": dict(train=train[0], test=test[0]), \"y\": dict(train=train[1], test=test[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "X = {\n",
    "\"normal\": data[\"X\"][\"train\"][data[\"y\"][\"train\"] == normal_class],\n",
    "\"anomaly\": data[\"X\"][\"train\"][data[\"y\"][\"train\"] != normal_class],\n",
    "}\n",
    "# y = {\"normal\": [0]*len(X[\"normal\"]), \"anomaly\": [1]*len(X[\"anomaly\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "X_test = {\n",
    "\"normal\": data[\"X\"][\"test\"][data[\"y\"][\"test\"] == normal_class],\n",
    "\"anomaly\": data[\"X\"][\"test\"][data[\"y\"][\"test\"] != normal_class],\n",
    "}\n",
    "# y_test = {\"normal\": [0]*len(X_test[\"normal\"]), \"anomaly\": [1]*len(X_test[\"anomaly\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce size of images for faster training and reduce to 0-1 range\n",
    "strides = (2,2) # (2,2) for 14x14 images; (4,4) for 7x7 images\n",
    "pool_size = (2,2)\n",
    "pool = tf.keras.layers.MaxPooling2D(pool_size=pool_size, strides=strides, padding=\"same\")\n",
    "\n",
    "# normal\n",
    "X_pool_normal = pool(tf.constant(X[\"normal\"].reshape(-1,28,28,1))).numpy().reshape(-1,7,7)/255.0\n",
    "X_pool_normal_test = pool(tf.constant(X_test[\"normal\"].reshape(-1,28,28,1))).numpy().reshape(-1,7,7)/255.0\n",
    "\n",
    "# anomaly\n",
    "X_pool_anomaly = pool(tf.constant(X[\"anomaly\"].reshape(-1,28,28,1))).numpy().reshape(-1,7,7)/255.0\n",
    "X_pool_anomaly_test = pool(tf.constant(X_test[\"anomaly\"].reshape(-1,28,28,1))).numpy().reshape(-1,14,14)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearagne pixels in zig-zag order (from https://arxiv.org/pdf/1605.05775.pdf)\n",
    "\n",
    "def zigzag_order(data):\n",
    "    data_zigzag = []\n",
    "    for x in data:\n",
    "        image = []\n",
    "        for i in x:\n",
    "            image.extend(i)\n",
    "        data_zigzag.append(image)\n",
    "    return np.asarray(data_zigzag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal = zigzag_order(X_pool_normal)\n",
    "test_normal = zigzag_order(X_pool_normal_test)\n",
    "\n",
    "train_anomaly = zigzag_order(X_pool_anomaly)\n",
    "test_anomaly = zigzag_order(X_pool_anomaly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take train_size samples from normal class for training\n",
    "train_size = 6000\n",
    "\n",
    "indices = list(range(len(train_normal)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "indices = indices[:train_size]\n",
    "train_normal = np.take(train_normal, indices, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training setup** &nbsp;\n",
    "- direct gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model parameters\n",
    "L = 49\n",
    "initializer = gramschmidt_init('normal', 0.5)\n",
    "key = jax.random.key(42)\n",
    "shape_method = 'noteven'\n",
    "bond_dim = 10\n",
    "phys_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MPS_initialize(L=L,\n",
    "                        initializer=initializer,\n",
    "                        key=key,\n",
    "                        shape_method=shape_method,\n",
    "                        bond_dim=bond_dim,\n",
    "                        phys_dim=phys_dim,\n",
    "                        cyclic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training parameters\n",
    "optimizer = optax.adam\n",
    "strategy = 'global'\n",
    "loss=neg_log_likelihood\n",
    "train_type=0\n",
    "embedding = trigonometric()\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model.configure(optimizer=optimizer, strategy=strategy, loss=loss, train_type=train_type, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping from flax\n",
    "from flax.training.early_stopping import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(min_delta=0, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.train(train_normal,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            embedding = embedding,\n",
    "            normalize=True,\n",
    "            earlystop=earlystop,\n",
    "            dtype=jnp.float64,\n",
    "            cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optax.exponential_decay(\n",
    "    init_value=1e-4,\n",
    "    transition_steps=1000,\n",
    "    decay_rate=0.5)\n",
    "\n",
    "# Combining gradient transforms using `optax.chain`.\n",
    "gradient_transforms = [\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    optax.scale_by_schedule(scheduler),  # Use the learning rate from the scheduler.\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if you can train first with global then sweeps\n",
    "model.configure(gradient_transforms=gradient_transforms, strategy='sweeps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.train(train_normal,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    embedding = embedding,\n",
    "                    normalize=True,\n",
    "                    earlystop=earlystop,\n",
    "                    dtype=jnp.float64,\n",
    "                    cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plt.plot(range(len(history['loss'])), history['loss'], label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_score = model.evaluate(test_anomaly, evaluate_type=0, return_list=True, dtype=jnp.float64, batch_size=1)\n",
    "normal_score = model.evaluate(test_normal, evaluate_type=0, return_list=True, dtype=jnp.float64, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_data(normal, anomaly):\n",
    "    true_val = np.concatenate((np.ones(anomaly.shape[0]), np.zeros(normal.shape[0])))\n",
    "    pred_val = np.concatenate((anomaly, normal))\n",
    "    fpr_loss, tpr_loss, threshold_loss = roc_curve(true_val, pred_val)\n",
    "    return fpr_loss, tpr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr = get_roc_data(normal_score, anomaly_score)\n",
    "auc_value = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot anomaly scores and ROC curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(anomaly_score, bins=100, histtype='step', label='anomaly')\n",
    "plt.hist(normal_score, bins=100, histtype='step', label='normal')\n",
    "plt.title('Anomaly score distribution')\n",
    "plt.legend()\n",
    "plt.text(0.5, -0.1, f'AUC Value: {auc_value}', ha='center', transform=plt.gca().transAxes)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_value)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Random guess line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
