{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": {
     "title": "# Classification for Bars-Stripes dataset"
    }
   },
   "source": [
    "# Classification for Linearly Separable dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from https://pennylane.ai/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.nn.initializers import *\n",
    "import optax\n",
    "from tn4ml.embeddings import *\n",
    "from tn4ml.util import *\n",
    "from tn4ml.models.model import *\n",
    "from tn4ml.models.smpo import *\n",
    "from tn4ml.initializers import *\n",
    "from tn4ml.loss import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ds] = qml.data.load(\"other\", name=\"linearly-separable\")\n",
    "\n",
    "inputs = np.array(ds.train['2']['inputs']) # points in 4-dimensional space\n",
    "labels = np.array(ds.train['2']['labels']).astype(int) # labels for the points above\n",
    "\n",
    "test_inputs = np.array(ds.test['2']['inputs']) # points in 4-dimensional space\n",
    "test_labels = np.array(ds.test['2']['labels']).astype(int) # labels for the points above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to range [0-1]\n",
    "inputs = (inputs - np.min(inputs, axis=0)) / (np.max(inputs, axis=0) - np.min(inputs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(inputs[labels==1][0], inputs[labels==1][1], color='blue', label='Label 1')\n",
    "plt.scatter(inputs[labels==-1][0], inputs[labels==-1][1], color='red', label='Label -1')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Visualization of Linearly Separable Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = integer_to_one_hot(labels, 2)\n",
    "test_labels = integer_to_one_hot(test_labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = inputs.shape[0]\n",
    "val_perc = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take val_size samples from normal class for validation (X% of training data)\n",
    "val_size = int(val_perc*train_size)\n",
    "train_size = int(train_size - val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(inputs)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size : train_size+val_size]\n",
    "\n",
    "# train data and validation inputs\n",
    "train_inputs = np.take(inputs, train_indices, axis=0)\n",
    "val_inputs = np.take(inputs, val_indices, axis=0)\n",
    "\n",
    "\n",
    "# train data and validation labels\n",
    "train_targets = np.take(labels, train_indices, axis=0)\n",
    "val_targets = np.take(labels, val_indices, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define TN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "n_classes = 2\n",
    "L = 2\n",
    "initializer = jax.nn.initializers.normal(0.5)\n",
    "key = jax.random.key(42)\n",
    "shape_method = 'noteven'\n",
    "bond_dim = 4\n",
    "phys_dim = (2, n_classes)\n",
    "spacing = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SMPO_initialize(L=L,\n",
    "                        initializer=initializer,\n",
    "                        key=key,\n",
    "                        shape_method=shape_method,\n",
    "                        spacing=spacing,\n",
    "                        bond_dim=bond_dim,\n",
    "                        phys_dim=phys_dim,\n",
    "                        cyclic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(*args, **kwargs):\n",
    "    return loss_wrapper_optax(optax.softmax_cross_entropy)(*args, **kwargs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "optimizer = optax.adam\n",
    "strategy = 'global'\n",
    "loss = cross_entropy\n",
    "train_type = 1\n",
    "#embedding = basis_quantum_encoding(basis={0: np.array([1, 0]), 1: np.array([0, 1])})\n",
    "embedding = trigonometric()\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Exponential decay of the learning rate.\n",
    "scheduler = optax.exponential_decay(\n",
    "    init_value=0.0001,\n",
    "    transition_steps=1000,\n",
    "    decay_rate=0.99)\n",
    "\n",
    "# Combining gradient transforms using `optax.chain`.\n",
    "gradient_transforms = [\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    optax.scale_by_schedule(scheduler),  # Use the learning rate from the scheduler.\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.configure(optimizer=optimizer, strategy=strategy, loss=loss, train_type=train_type, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping from flax\n",
    "from flax.training.early_stopping import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(min_delta=0, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.train(train_inputs.reshape(train_inputs.shape[0], 2),\n",
    "                    targets = train_targets,\n",
    "                    val_inputs=val_inputs.reshape(val_inputs.shape[0], 2),\n",
    "                    val_targets = val_targets,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    embedding = embedding,\n",
    "                    earlystop=earlystop,\n",
    "                    normalize = True,\n",
    "                    cache=True,\n",
    "                    dtype = jnp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(len(model.history['loss'])), model.history['loss'], label='train')\n",
    "plt.plot(range(len(model.history['val_loss'])), model.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tn4ml.models.model import _batch_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "correct_predictions = 0; total_loss = 0\n",
    "\n",
    "for batch_data in _batch_iterator(test_inputs.reshape(test_inputs.shape[0], 2), test_labels, batch_size=batch_size):\n",
    "    x, y = batch_data\n",
    "    x = jnp.array(x, dtype=jnp.float64)\n",
    "    y = jnp.array(y)\n",
    "\n",
    "    y_pred = jnp.squeeze(jnp.array(jax.vmap(model.predict, in_axes=(0, None, None))(x, embedding, False)[0]))\n",
    "    y_pred\n",
    "    predicted = jnp.argmax(y_pred, axis=-1)\n",
    "    true = jnp.argmax(y, axis=-1)\n",
    "\n",
    "    correct_predictions += jnp.sum(predicted == true).item() / batch_size\n",
    "\n",
    "accuracy = correct_predictions / (len(test_inputs)//batch_size)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
