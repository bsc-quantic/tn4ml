{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1836a26",
   "metadata": {},
   "source": [
    "## Global update with `Dask`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4654aa7f",
   "metadata": {},
   "source": [
    "`Dask` is a framework that allows to add parallelization to code in Python. To install the extra dependencies (not added in `pyproject.toml` yet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a933798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -m pip install dask distributed --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6b3cec",
   "metadata": {},
   "source": [
    "`graphviz` (or `cytoscape`) is also needed to visualize the task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deb4859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "client = Client()\n",
    "client # Displays the information of the local client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c50b3",
   "metadata": {},
   "source": [
    "This creates automatically a Dask 'Client' given by our system specificaition. One can then go to the link in the previous block to see `Dask dashboard`, which enables us to see the task stream and other useful information live during the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727a1cd",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256dc7cc-30d1-435e-9b01-76fe7d903670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "from multiprocessing import pool\n",
    "import os\n",
    "import sys\n",
    "import json \n",
    "import time\n",
    "import dask\n",
    "from dask.distributed import Client, wait, performance_report # LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tnad.FeatureMap as fm\n",
    "from tnad.losses import loss_miss, loss_reg\n",
    "from tnad.gradients import gradient_miss, gradient_reg\n",
    "from tnad.optimization import load_mnist_train_data, data_preprocessing\n",
    "from tnad import smpo\n",
    "import tnad.procedures as p\n",
    "import math\n",
    "import quimb.tensor as qtn\n",
    "import quimb as qu\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed8bb0",
   "metadata": {},
   "source": [
    "`procedures.py` functions adapted to Dask framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a034bee-d401-4370-99e1-b18f25861330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def local_update_sweep_dyncanonization_renorm(P, n_epochs, n_iters, data, batch_size, alpha, lamda_init, lamda_init_2, bond_dim, decay_rate=None, expdecay_tol=None):\n",
    "    N_features = P.nsites\n",
    "\n",
    "    loss_array = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for it in (pbar := tqdm(range(n_iters))):        \n",
    "            pbar.set_description(\"Epoch #\"+str(epoch)+\", sample in batch:\")\n",
    "            # define sweeps\n",
    "            sweeps = itertools.chain(zip(list(range(0,N_features-1)), list(range(1,N_features))), reversed(list(zip(list(range(1,N_features)),list(range(0,N_features-1))))))\n",
    "            for sweep_it, sites in enumerate(sweeps):\n",
    "                [sitel, siter] = sites\n",
    "                site_tags = [P.site_tag(site) for site in sites]\n",
    "                # canonize P with root in sites\n",
    "                ortog_center = sites\n",
    "                P.canonize(sites, cur_orthog=ortog_center)\n",
    "                # copy P as reference\n",
    "                P_ref = P.copy(deep=True)\n",
    "                # pop site tensor\n",
    "                [origl, origr] = P.select_tensors(site_tags, which=\"any\")\n",
    "                tensor_orig = origl & origr ^ all\n",
    "                # memorize bond between 2 selected sites\n",
    "                bond_ind_removed = P.bond(site_tags[0], site_tags[1])\n",
    "\n",
    "                #virtual bonds\n",
    "                #    left\n",
    "                if sitel == 0 or (sitel == N_features-1 and sitel>siter): vindl = []\n",
    "                elif sitel>0 and sitel<siter: vindl = [P.bond(sitel-1, sitel)]\n",
    "                else: vindl = [P.bond(sitel, sitel+1)]\n",
    "                #    right\n",
    "                if siter == N_features - 1 or (siter == 0 and siter<sitel): vindr = []\n",
    "                elif siter < N_features-1 and siter>sitel: vindr = [P.bond(siter, siter+1)]\n",
    "                else: vindr = [P.bond(siter-1, siter)]\n",
    "\n",
    "                # remove site tags of poped sites\n",
    "                P.delete(site_tags, which=\"any\")\n",
    "\n",
    "                grad_miss=0; loss_miss_batch=0\n",
    "                for sample in data[it]:\n",
    "                    # create MPS for input sample\n",
    "                    phi, _ = fm.embed(sample.flatten(), fm.trigonometric)\n",
    "                    \n",
    "                    #calculate loss\n",
    "                    loss_miss_batch += dask.delayed(loss_miss)(phi, P_ref)\n",
    "                    \n",
    "                    #calculate gradient\n",
    "                    grad_miss += dask.delayed(gradient_miss)(phi, P_ref, P, sites)\n",
    "                # total loss\n",
    "                \n",
    "                loss_miss_batch = loss_miss_batch.compute()\n",
    "                grad_miss = grad_miss.compute()\n",
    "\n",
    "                loss = (1/batch_size)*(loss_miss_batch)\n",
    "                loss_array.append(loss)\n",
    "\n",
    "                # gradient of loss miss\n",
    "                grad_miss.drop_tags()\n",
    "                grad_miss.add_tag(site_tags[0]); grad_miss.add_tag(site_tags[1])\n",
    "                # gradient of loss reg\n",
    "                # grad_regular = gradient_reg(P_ref, P, alpha, sites, N_features)\n",
    "                # if grad_regular != 0:\n",
    "                #     grad_regular.drop_tags()\n",
    "                #     grad_regular.add_tag(site_tags[0]); grad_regular.add_tag(site_tags[1])\n",
    "                # total gradient\n",
    "                total_grad = (1/batch_size)*grad_miss\n",
    "\n",
    "                # update tensor\n",
    "                if epoch >= expdecay_tol:\n",
    "                    if decay_rate != None:\n",
    "                        # exp. decay of lamda\n",
    "                        if epoch == exp_decay_tol: lamda = lamda_init_2\n",
    "                        else: lamda = lamda_init_2*math.pow((1 - decay_rate/100),epoch)\n",
    "                        tensor_new = tensor_orig - lamda*total_grad\n",
    "                else:\n",
    "                    tensor_new = tensor_orig - lamda_init*total_grad\n",
    "\n",
    "                # normalize updated tensor\n",
    "                tensor_new.normalize(inplace=True)\n",
    "\n",
    "                # split updated tensor in 2 tensors\n",
    "                lower_ind = [f'b{sitel}'] if f'b{sitel}' in P.lower_inds else []\n",
    "                [tensorl, tensorr] = tensor_new.split(get=\"tensors\", left_inds=[*vindl, P.upper_ind(sitel), *lower_ind], bond_ind=bond_ind_removed, max_bond=bond_dim)\n",
    "\n",
    "                # link new tensors to P back\n",
    "                for site, tensor in zip(sites, [tensorl, tensorr]):\n",
    "                    tensor.drop_tags()\n",
    "                    tensor.add_tag(P.site_tag(site))\n",
    "                    P.add_tensor(tensor)\n",
    "\n",
    "                    \n",
    "def get_sample_grad(sample, embed_func, P, P_rem, tensor):\n",
    "    # create MPS for input sample\n",
    "    phi, _ = fm.embed(sample.flatten(), embed_func)\n",
    "\n",
    "    #calculate gradient\n",
    "    grad_miss = gradient_miss(phi, P, P_rem, [tensor])\n",
    "    return grad_miss\n",
    "\n",
    "def get_sample_loss(sample, embed_func, P):\n",
    "    # create MPS for input sample\n",
    "    phi, _ = fm.embed(sample.flatten(), embed_func)\n",
    "\n",
    "    #calculate loss\n",
    "    loss_miss_batch = loss_miss(phi, P)\n",
    "    return loss_miss_batch\n",
    "\n",
    "def get_total_grad(P, tensor, data, embed_func, batch_size, alpha):\n",
    "    P_rem = P.copy(deep=True)\n",
    "    \n",
    "    site_tag = P_rem.site_tag(tensor)\n",
    "    # remove site tag of poped sites\n",
    "    P_rem.delete(site_tag, which=\"any\")\n",
    "\n",
    "    # parallelize\n",
    "    grad_miss = []\n",
    "    for i, sample in enumerate(data):\n",
    "        output_per_sample = get_sample_grad(sample, embed_func, P, P_rem, tensor)\n",
    "        grad_miss.append(output_per_sample)\n",
    "    \n",
    "    # gradient of loss miss\n",
    "    grad_miss = sum(grad_miss)\n",
    "    grad_miss.drop_tags()\n",
    "    grad_miss.add_tag(site_tag)\n",
    "    # gradient of loss reg\n",
    "    grad_regular = gradient_reg(P, P_rem, alpha, [tensor])\n",
    "    if grad_regular != 0:\n",
    "        grad_regular.drop_tags()\n",
    "        grad_regular.add_tag(site_tag)\n",
    "    # total gradient\n",
    "    total_grad = (1/batch_size)*(grad_miss) + grad_regular\n",
    "    \n",
    "    return total_grad\n",
    "\n",
    "def global_update_costfuncnorm(P, n_epochs, n_iters, data, batch_size, alpha, lamda_init, lamda_init_2, bond_dim, decay_rate=None, expdecay_tol=None):\n",
    "    loss_array = []\n",
    "    loss_reg_array = []\n",
    "    loss_miss_array = []\n",
    "    n_tensors = P.nsites\n",
    "    # P = dask.delayed(P)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for it in (pbar := tqdm(range(n_iters))):             \n",
    "            pbar.set_description(\"Epoch #\"+str(epoch)+\", sample in batch:\")\n",
    "            \n",
    "            # parallelize\n",
    "            grad_per_tensor=[]\n",
    "            for tensor in range(n_tensors):\n",
    "                embed_func = fm.trigonometric\n",
    "                output_per_tensor = dask.delayed(get_total_grad)(P, tensor, data[it], embed_func, batch_size, alpha) # get grad per tensor\n",
    "                grad_per_tensor.append(output_per_tensor)\n",
    "            grad_per_tensor = list(dask.compute(*grad_per_tensor))\n",
    "            \n",
    "            # get loss per sample\n",
    "            loss_miss = 0\n",
    "            for i, sample in enumerate(data[it]):\n",
    "                embed_func = fm.trigonometric\n",
    "                output_per_sample = dask.delayed(get_sample_loss)(sample, embed_func, P)\n",
    "                loss_miss += output_per_sample\n",
    "                \n",
    "            # get total loss\n",
    "            total_loss = (1/batch_size)*(loss_miss) + dask.delayed(loss_reg)(P, alpha)\n",
    "            total_loss_miss = (1/batch_size)*(loss_miss)\n",
    "            total_loss_reg = dask.delayed(loss_reg)(P, alpha)\n",
    "            \n",
    "            \n",
    "            loss_array.append(total_loss)\n",
    "            loss_reg_array.append(total_loss_reg)\n",
    "            loss_miss_array.append(total_loss_miss)\n",
    "            \n",
    "            loss_array = list(dask.compute(*loss_array))\n",
    "            loss_reg_array = list(dask.compute(*loss_reg_array))\n",
    "            loss_miss_array = list(dask.compute(*loss_miss_array))  \n",
    "            \n",
    "            # update P\n",
    "            # no need to parallelize\n",
    "            futures = []\n",
    "            for tensor in range(n_tensors):\n",
    "                site_tag = P.site_tag(tensor)\n",
    "                tensor_orig = dask.compute(P.select_tensors(site_tag, which=\"any\")[0])[0]\n",
    "\n",
    "                if epoch >= expdecay_tol:\n",
    "                    if decay_rate != None:\n",
    "                        # exp. decay of lamda\n",
    "                        if epoch == exp_decay_tol: lamda = lamda_init_2\n",
    "                        else: lamda = lamda_init_2*math.pow((1 - decay_rate/100),epoch)\n",
    "                        tensor_orig.modify(data=tensor_orig.data - lamda*grad_per_tensor[tensor].transpose_like(tensor_orig).data)\n",
    "                else:\n",
    "                    tensor_orig.modify(data=tensor_orig.data - lamda_init*grad_per_tensor[tensor].transpose_like(tensor_orig).data)            \n",
    "  \n",
    "\n",
    "    return P, list(loss_array), list(loss_reg_array), list(loss_miss_array)\n",
    "\n",
    "def train_SMPO(data, spacing, n_epochs, alpha, opt_procedure, lamda_init=2e-5, lamda_init_2=2e-3, decay_rate=None, expdecay_tol=None, bond_dim=4, init_func='normal', scale=0.5, batch_size=32, seed: int = None):\n",
    "    \n",
    "    train_data = np.array(data)\n",
    "    N_features = train_data.shape[1]*train_data.shape[2]\n",
    "    train_data_batched = np.array(np.split(train_data, batch_size))\n",
    "    n_iters = int(train_data.shape[0]/batch_size)\n",
    "    \n",
    "    # initialize P\n",
    "    P_orig = smpo.SpacedMatrixProductOperator.rand(n=N_features, spacing=spacing, bond_dim=bond_dim, init_func=init_func, scale=scale, seed=seed)\n",
    "    P = P_orig.copy(deep=True)\n",
    "    \n",
    "    P, loss_array, loss_reg_array, loss_miss_array = opt_procedure(P, n_epochs, n_iters, train_data_batched, batch_size, alpha, lamda_init=lamda_init, lamda_init_2=lamda_init_2, bond_dim=bond_dim, decay_rate=decay_rate, expdecay_tol=expdecay_tol)\n",
    "    return P, loss_array, loss_reg_array, loss_miss_array\n",
    "\n",
    "# args:\n",
    "train_size = 64\n",
    "batch_size = 32\n",
    "strides = (2,2)\n",
    "pool_size = (2,2)\n",
    "padding = 'same'\n",
    "reduced_shape = (14,14)\n",
    "opt_procedure = 'global_update_costfuncnorm' # 'global_update_costfuncnorm'\n",
    "spacing = 8\n",
    "n_epochs = 3\n",
    "alpha = 0.4\n",
    "lambda_init = 2e-5\n",
    "lambda_init_2 = lambda_init # 2e-3\n",
    "decay_rate = 0.01\n",
    "expdecay_tol = 20\n",
    "bond_dim = 5\n",
    "init_func = 'normal'\n",
    "scale_init_p = 0.5\n",
    "\n",
    "params = {\n",
    "    'train_size': train_size, \n",
    "    'batch_size': batch_size,\n",
    "    'strides': strides,\n",
    "    'pool_size': pool_size,\n",
    "    'padding': padding,\n",
    "    'reduced_shape': reduced_shape,\n",
    "    'opt_procedure': opt_procedure,\n",
    "    'spacing': spacing,\n",
    "    'n_epochs': n_epochs,\n",
    "    'alpha': alpha,\n",
    "    'lambda_init': lambda_init,\n",
    "    'lambda_init_2': lambda_init_2,\n",
    "    'decay_rate': decay_rate,\n",
    "    'expdecay_tol': expdecay_tol,\n",
    "    'bond_dim': bond_dim,\n",
    "    'init_func': init_func,\n",
    "    'scale_init_p': scale_init_p\n",
    "    }\n",
    "\n",
    "results_folder = \"/gpfs/scratch/bsc21/bsc21504/tnad/v_30-09-22/output/dask/128_trainsize/results\"\n",
    "reports_folder = \"/gpfs/scratch/bsc21/bsc21504/tnad/v_30-09-22/output/dask/128_trainsize/preports\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a5293",
   "metadata": {},
   "source": [
    "We can now take a look at the `train_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd7acb-16c4-465e-b456-ff4b9bed6bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tnad.optimization import load_mnist_train_data, data_preprocessing\n",
    "import tnad.procedures as p\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = load_mnist_train_data(train_size=train_size) # seed=123456\n",
    "data = data_preprocessing(train_data, strides=strides, pool_size=pool_size, padding=padding, reduced_shape=reduced_shape)\n",
    "plt.imshow(train_data[0,:,:], interpolation='nearest', cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4145b7-3616-45b5-ad55-50a702ca4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_data = load_mnist_train_data(train_size=train_size)\n",
    "data = data_preprocessing(train_data, strides=strides, pool_size=pool_size, padding=padding, reduced_shape=reduced_shape)\n",
    "\n",
    "if opt_procedure == 'local_update_sweep_dyncanonization_renorm':\n",
    "    opt_procedure = local_update_sweep_dyncanonization_renorm\n",
    "\n",
    "elif opt_procedure == 'global_update_costfuncnorm':\n",
    "    opt_procedure = global_update_costfuncnorm\n",
    "\n",
    "P, loss_array, loss_reg_array, loss_miss_array = train_SMPO(data, spacing, n_epochs, alpha, opt_procedure, lambda_init, lambda_init_2, decay_rate, expdecay_tol, bond_dim, init_func, scale_init_p, batch_size)\n",
    "wait(loss_array)\n",
    "loss_array_values = dask.compute(*loss_array)\n",
    "\n",
    "computation_time = time.time()-start_time\n",
    "print(\"computation time: \", computation_time, flush=True)\n",
    "print(loss_array_values)\n",
    "\n",
    "delayed_time = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebedc892-4abb-4940-87a3-983eb4285b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot\n",
    "markersize = 4.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "ax.plot(range(0, len(loss_array)), loss_array, \"o\", linestyle=\"--\", markersize=markersize, label=r\"total loss\")\n",
    "ax.plot(range(0, len(loss_reg_array)), loss_reg_array, \"s\", linestyle=\"--\", markersize=markersize, label=r\"loss reg\")\n",
    "ax.plot(range(0, len(loss_miss_array)), loss_miss_array, \"d\", linestyle=\"--\", markersize=markersize, label=r\"loss miss\")\n",
    "\n",
    "ax.set_xlabel(\"epoch\", fontsize=16)\n",
    "ax.set_ylabel(\"loss\", fontsize=16)\n",
    "\n",
    "ax.tick_params(labelsize=14)\n",
    "\n",
    "plt.subplots_adjust(\n",
    "    top=0.97,\n",
    "    bottom=0.14,\n",
    "    left=0.13,\n",
    "    right=0.97,\n",
    "    hspace=0.,\n",
    "    wspace=0.\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"best\", prop={'size': 17}, handlelength=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
